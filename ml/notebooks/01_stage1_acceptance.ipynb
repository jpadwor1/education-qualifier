{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4d4bbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ No sample found. Building sample from raw CSVs (one-time cost).\n",
      "Sampling 500,000 rows total -> 37,792 accepted, 462,208 rejected\n",
      "✅ Saved sample to: ..\\data_sample\\stage1_accept_reject_sample.parquet\n",
      "\n",
      "--- Sample Summary ---\n",
      "Shape: (500000, 1)\n",
      "Duplicate columns: []\n",
      "\n",
      "Label distribution:\n",
      "is_accepted\n",
      "0    462208\n",
      "1     37792\n",
      "Name: count, dtype: int64\n",
      "is_accepted\n",
      "0    0.924416\n",
      "1    0.075584\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Dtypes head:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "is_accepted    int8\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missingness (top 20):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "is_accepted    0.0\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Stage 1: Accepted vs Rejected (Data Prep + Persisted Sample)\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 140)\n",
    "\n",
    "# ----------------------------\n",
    "# Paths\n",
    "# ----------------------------\n",
    "DATA_DIR = Path(\"../data\")                # raw CSVs live here (gitignored)\n",
    "SAMPLE_DIR = Path(\"../data_sample\")       # lightweight samples live here\n",
    "SAMPLE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ACCEPTED_CSV = DATA_DIR / \"accepted_2007_to_2018Q4.csv\"\n",
    "REJECTED_CSV = DATA_DIR / \"rejected_2007_to_2018Q4.csv\"\n",
    "\n",
    "SAMPLE_PARQUET = SAMPLE_DIR / \"stage1_accept_reject_sample.parquet\"\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "SAMPLE_N = 500_000     # adjust: 200_000 (fast), 500_000 (good), 1_000_000 (heavier)\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def strip_pct(x):\n",
    "    \"\"\"Convert strings like '13.56%' to float 13.56; return NaN if not convertible.\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if s.endswith(\"%\"):\n",
    "            s = s[:-1].strip()\n",
    "        try:\n",
    "            return float(s)\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "    return x\n",
    "\n",
    "def safe_drop(df, cols):\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    if cols:\n",
    "        return df.drop(columns=cols)\n",
    "    return df\n",
    "\n",
    "# ----------------------------\n",
    "# Load or build sample\n",
    "# ----------------------------\n",
    "if SAMPLE_PARQUET.exists():\n",
    "    print(f\"✅ Loading existing sample: {SAMPLE_PARQUET}\")\n",
    "    df1 = pd.read_parquet(SAMPLE_PARQUET)\n",
    "else:\n",
    "    print(\"⏳ No sample found. Building sample from raw CSVs (one-time cost).\")\n",
    "\n",
    "    # 1) Load raw files\n",
    "    acc = pd.read_csv(ACCEPTED_CSV, low_memory=False)\n",
    "    rej = pd.read_csv(REJECTED_CSV, low_memory=False)\n",
    "\n",
    "    # 2) Add labels\n",
    "    acc[\"is_accepted\"] = 1\n",
    "    rej[\"is_accepted\"] = 0\n",
    "\n",
    "    # 3) Intersection columns (IMPORTANT: do NOT duplicate label)\n",
    "    common = sorted(set(acc.columns).intersection(set(rej.columns)))\n",
    "    keep_cols = [c for c in common if c != \"is_accepted\"] + [\"is_accepted\"]\n",
    "\n",
    "    acc_s1 = acc[keep_cols].copy()\n",
    "    rej_s1 = rej[keep_cols].copy()\n",
    "\n",
    "    # Release memory\n",
    "    del acc, rej\n",
    "\n",
    "    # 4) Concatenate\n",
    "    df_full = pd.concat([acc_s1, rej_s1], ignore_index=True)\n",
    "\n",
    "    # 5) Ensure no duplicate columns remain\n",
    "    df_full = df_full.loc[:, ~df_full.columns.duplicated()].copy()\n",
    "    assert df_full[\"is_accepted\"].ndim == 1, \"❌ is_accepted is not 1-D (duplicate column names still exist).\"\n",
    "\n",
    "    # 6) Optional: drop \"UI-unfriendly\" / PII-ish / free-text columns if present\n",
    "    DROP_COLS = [\n",
    "        \"zip_code\", \"addr_state\", \"state\",\n",
    "        \"emp_title\", \"title\", \"desc\",\n",
    "        \"url\", \"id\", \"member_id\"\n",
    "    ]\n",
    "    df_full = safe_drop(df_full, DROP_COLS)\n",
    "\n",
    "    # 7) Basic type cleaning (only if columns exist)\n",
    "    for col in [\"dti\", \"revol_util\", \"int_rate\"]:\n",
    "        if col in df_full.columns:\n",
    "            df_full[col] = df_full[col].apply(strip_pct)\n",
    "\n",
    "    # term \"36 months\" -> 36\n",
    "    if \"term\" in df_full.columns:\n",
    "        df_full[\"term\"] = (\n",
    "            df_full[\"term\"]\n",
    "            .astype(str)\n",
    "            .str.extract(r\"(\\d+)\")\n",
    "            .astype(float)\n",
    "        )\n",
    "\n",
    "    # Force label dtype\n",
    "    df_full[\"is_accepted\"] = pd.to_numeric(df_full[\"is_accepted\"], errors=\"coerce\").astype(\"int8\")\n",
    "\n",
    "    # 8) Stratified sampling WITHOUT sklearn (fast and memory-friendly)\n",
    "    df_pos = df_full[df_full[\"is_accepted\"] == 1]\n",
    "    df_neg = df_full[df_full[\"is_accepted\"] == 0]\n",
    "\n",
    "    pos_ratio = len(df_pos) / len(df_full)\n",
    "    n_pos = int(SAMPLE_N * pos_ratio)\n",
    "    n_neg = SAMPLE_N - n_pos\n",
    "\n",
    "    print(f\"Sampling {SAMPLE_N:,} rows total -> {n_pos:,} accepted, {n_neg:,} rejected\")\n",
    "\n",
    "    df_pos_s = df_pos.sample(n=min(n_pos, len(df_pos)), random_state=RANDOM_STATE)\n",
    "    df_neg_s = df_neg.sample(n=min(n_neg, len(df_neg)), random_state=RANDOM_STATE)\n",
    "\n",
    "    df1 = pd.concat([df_pos_s, df_neg_s], ignore_index=True).sample(frac=1.0, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "    # Release memory\n",
    "    del df_full, df_pos, df_neg, df_pos_s, df_neg_s\n",
    "\n",
    "    # 9) Save sample as Parquet for fast reloads\n",
    "    # Requires pyarrow or fastparquet:\n",
    "    # pip install pyarrow\n",
    "    df1.to_parquet(SAMPLE_PARQUET, index=False)\n",
    "    print(f\"✅ Saved sample to: {SAMPLE_PARQUET}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Sanity checks\n",
    "# ----------------------------\n",
    "print(\"\\n--- Sample Summary ---\")\n",
    "print(\"Shape:\", df1.shape)\n",
    "\n",
    "dupes = df1.columns[df1.columns.duplicated()].tolist()\n",
    "print(\"Duplicate columns:\", dupes)\n",
    "\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df1[\"is_accepted\"].value_counts())\n",
    "print(df1[\"is_accepted\"].value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nDtypes head:\")\n",
    "display(df1.dtypes.head(30))\n",
    "\n",
    "print(\"\\nMissingness (top 20):\")\n",
    "missing = (df1.isna().mean().sort_values(ascending=False).head(20))\n",
    "display(missing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212ba0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric cols: 0 | Categorical cols: 0 | Total: 0\n",
      "Splits: (350000, 0) (75000, 0) (75000, 0)\n",
      "Train pos rate: 0.07558285714285715\n",
      "\n",
      "=== Training: logreg (with L1 feature selection) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\john_\\AppData\\Roaming\\Python\\Python314\\site-packages\\sklearn\\linear_model\\_logistic.py:1135: FutureWarning: 'penalty' was deprecated in version 1.8 and will be removed in 1.10. To avoid this warning, leave 'penalty' set to its default value and use 'l1_ratio' or 'C' instead. Use l1_ratio=0 instead of penalty='l2', l1_ratio=1 instead of penalty='l1', and C=np.inf instead of penalty=None.\n",
      "  warnings.warn(\n",
      "C:\\Users\\john_\\AppData\\Roaming\\Python\\Python314\\site-packages\\sklearn\\linear_model\\_logistic.py:1160: UserWarning: Inconsistent values: penalty=l1 with l1_ratio=0.0. penalty is deprecated. Please use l1_ratio only.\n",
      "  warnings.warn(\n",
      "C:\\Users\\john_\\AppData\\Roaming\\Python\\Python314\\site-packages\\sklearn\\linear_model\\_logistic.py:1184: FutureWarning: 'n_jobs' has no effect since 1.8 and will be removed in 1.10. You provided 'n_jobs=-1', please leave it unspecified.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 feature(s) (shape=(350000, 0)) while a minimum of 1 is required by LogisticRegression.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 128\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Training: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (with L1 feature selection) ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    122\u001b[39m pipe = Pipeline(steps=[\n\u001b[32m    123\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mprep\u001b[39m\u001b[33m\"\u001b[39m, preprocess),\n\u001b[32m    124\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mselect\u001b[39m\u001b[33m\"\u001b[39m, selector),\n\u001b[32m    125\u001b[39m     (\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, clf)\n\u001b[32m    126\u001b[39m ])\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m val_proba = pipe.predict_proba(X_val)[:, \u001b[32m1\u001b[39m]\n\u001b[32m    131\u001b[39m val_auc = roc_auc_score(y_val, val_proba)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\sklearn\\base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\sklearn\\pipeline.py:613\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    606\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    607\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe `transform_input` parameter can only be set if metadata \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    608\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrouting is enabled. You can enable metadata routing using \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`sklearn.set_config(enable_metadata_routing=True)`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m     )\n\u001b[32m    612\u001b[39m routed_params = \u001b[38;5;28mself\u001b[39m._check_method_params(method=\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, props=params)\n\u001b[32m--> \u001b[39m\u001b[32m613\u001b[39m Xt = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[33m\"\u001b[39m\u001b[33mPipeline\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m._log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.steps) - \u001b[32m1\u001b[39m)):\n\u001b[32m    615\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\sklearn\\pipeline.py:547\u001b[39m, in \u001b[36mPipeline._fit\u001b[39m\u001b[34m(self, X, y, routed_params, raw_params)\u001b[39m\n\u001b[32m    540\u001b[39m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[32m    541\u001b[39m step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    542\u001b[39m     step_idx=step_idx,\n\u001b[32m    543\u001b[39m     step_params=routed_params[name],\n\u001b[32m    544\u001b[39m     all_params=raw_params,\n\u001b[32m    545\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m547\u001b[39m X, fitted_transformer = \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPipeline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[32m    557\u001b[39m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.steps[step_idx] = (name, fitted_transformer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\joblib\\memory.py:326\u001b[39m, in \u001b[36mNotMemorizedFunc.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\sklearn\\pipeline.py:1484\u001b[39m, in \u001b[36m_fit_transform_one\u001b[39m\u001b[34m(transformer, X, y, weight, message_clsname, message, params)\u001b[39m\n\u001b[32m   1482\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[32m   1483\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[33m\"\u001b[39m\u001b[33mfit_transform\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1484\u001b[39m         res = \u001b[43mtransformer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit_transform\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1485\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1486\u001b[39m         res = transformer.fit(X, y, **params.get(\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m, {})).transform(\n\u001b[32m   1487\u001b[39m             X, **params.get(\u001b[33m\"\u001b[39m\u001b[33mtransform\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m   1488\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\sklearn\\base.py:910\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    907\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, **fit_params).transform(X)\n\u001b[32m    908\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    909\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m910\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m.transform(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\sklearn\\base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\sklearn\\feature_selection\\_from_model.py:385\u001b[39m, in \u001b[36mSelectFromModel.fit\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    382\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    383\u001b[39m         \u001b[38;5;66;03m# TODO(SLEP6): remove when metadata routing cannot be disabled.\u001b[39;00m\n\u001b[32m    384\u001b[39m         \u001b[38;5;28mself\u001b[39m.estimator_ = clone(\u001b[38;5;28mself\u001b[39m.estimator)\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mestimator_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.estimator_, \u001b[33m\"\u001b[39m\u001b[33mfeature_names_in_\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m.feature_names_in_ = \u001b[38;5;28mself\u001b[39m.estimator_.feature_names_in_\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\sklearn\\base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\sklearn\\linear_model\\_logistic.py:1191\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1188\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1189\u001b[39m     _dtype = [np.float64, np.float32]\n\u001b[32m-> \u001b[39m\u001b[32m1191\u001b[39m X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mliblinear\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msag\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msaga\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1200\u001b[39m n_features = X.shape[\u001b[32m1\u001b[39m]\n\u001b[32m   1201\u001b[39m check_classification_targets(y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\sklearn\\utils\\validation.py:2919\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2917\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m   2918\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2919\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2920\u001b[39m     out = X, y\n\u001b[32m   2922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\sklearn\\utils\\validation.py:1314\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1309\u001b[39m         estimator_name = _check_estimator_name(estimator)\n\u001b[32m   1310\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1311\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires y to be passed, but the target y is None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1312\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1314\u001b[39m X = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1316\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1317\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1318\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1319\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1321\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1322\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1331\u001b[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001b[32m   1333\u001b[39m check_consistent_length(X, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python314\\site-packages\\sklearn\\utils\\validation.py:1106\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1104\u001b[39m     n_features = array.shape[\u001b[32m1\u001b[39m]\n\u001b[32m   1105\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_features < ensure_min_features:\n\u001b[32m-> \u001b[39m\u001b[32m1106\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1107\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m feature(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m) while\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1108\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m a minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1109\u001b[39m             % (n_features, array.shape, ensure_min_features, context)\n\u001b[32m   1110\u001b[39m         )\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_non_negative:\n\u001b[32m   1113\u001b[39m     whom = input_name\n",
      "\u001b[31mValueError\u001b[39m: Found array with 0 feature(s) (shape=(350000, 0)) while a minimum of 1 is required by LogisticRegression."
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Stage 1 Consolidated Training Cell (with Feature Selection)\n",
    "# - Uses df1 sample already loaded (500k)\n",
    "# - Train/Val/Test split\n",
    "# - Preprocess: impute + scale numeric, impute + one-hot categorical\n",
    "# - Feature selection: L1 Logistic Regression via SelectFromModel\n",
    "# - Model candidates: Logistic Regression, Calibrated Linear SVM\n",
    "# - Select best by Validation ROC-AUC\n",
    "# - Evaluate on Test\n",
    "# - Save:\n",
    "#     ../artifacts/stage1_pipeline.pkl\n",
    "#     ../artifacts/stage1_metrics.json\n",
    "#     ../artifacts/metadata.json\n",
    "# ============================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "RANDOM_STATE = 42\n",
    "ART_DIR = Path(\"../artifacts\")\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Safety: ensure no duplicate cols and label is 1-D numeric\n",
    "# ----------------------------\n",
    "df1 = df1.loc[:, ~df1.columns.duplicated()].copy()\n",
    "assert \"is_accepted\" in df1.columns, \"df1 must contain is_accepted\"\n",
    "\n",
    "# If somehow duplicated label slipped in earlier, keep first\n",
    "if isinstance(df1[\"is_accepted\"], pd.DataFrame):\n",
    "    df1[\"is_accepted\"] = df1[\"is_accepted\"].iloc[:, 0]\n",
    "\n",
    "df1[\"is_accepted\"] = pd.to_numeric(df1[\"is_accepted\"], errors=\"coerce\").astype(\"int8\")\n",
    "\n",
    "# ----------------------------\n",
    "# X/y\n",
    "# ----------------------------\n",
    "y = df1[\"is_accepted\"].copy()\n",
    "X = df1.drop(columns=[\"is_accepted\"]).copy()\n",
    "\n",
    "# Drop columns that are all-missing (helps preprocessing stability)\n",
    "all_nan_cols = [c for c in X.columns if X[c].isna().all()]\n",
    "if all_nan_cols:\n",
    "    print(f\"Dropping {len(all_nan_cols)} all-NaN columns (sample):\", all_nan_cols[:10])\n",
    "    X = X.drop(columns=all_nan_cols)\n",
    "\n",
    "# Identify numeric/categorical columns\n",
    "num_cols = X.select_dtypes(include=[\"number\", \"bool\"]).columns.tolist()\n",
    "cat_cols = [c for c in X.columns if c not in num_cols]\n",
    "\n",
    "print(f\"Numeric cols: {len(num_cols)} | Categorical cols: {len(cat_cols)} | Total: {X.shape[1]}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Split (stratified)\n",
    "# ----------------------------\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=RANDOM_STATE, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"Splits:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "print(\"Train pos rate:\", float(y_train.mean()))\n",
    "\n",
    "# ----------------------------\n",
    "# Preprocess (sparse-friendly)\n",
    "# ----------------------------\n",
    "numeric_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),  # must be False for sparse\n",
    "])\n",
    "\n",
    "categorical_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)),\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_pipe, num_cols),\n",
    "        (\"cat\", categorical_pipe, cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=0.3\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Feature Selection (L1 Logistic Regression)\n",
    "# - Runs AFTER preprocessing (so it can select among one-hot columns)\n",
    "# - Uses only TRAIN data inside fit\n",
    "# ----------------------------\n",
    "selector = SelectFromModel(\n",
    "    estimator=LogisticRegression(\n",
    "        penalty=\"l1\",\n",
    "        solver=\"saga\",\n",
    "        class_weight=\"balanced\",\n",
    "        max_iter=500,\n",
    "        C=0.5,         # smaller -> more aggressive selection\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    threshold=\"median\"  # keep features above median abs coef\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# Candidate models\n",
    "# ----------------------------\n",
    "candidates = {\n",
    "    \"logreg\": LogisticRegression(\n",
    "        max_iter=400,\n",
    "        solver=\"lbfgs\",\n",
    "        class_weight=\"balanced\",\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    \"calibrated_svm\": CalibratedClassifierCV(\n",
    "        estimator=LinearSVC(class_weight=\"balanced\", random_state=RANDOM_STATE),\n",
    "        method=\"sigmoid\",\n",
    "        cv=3\n",
    "    )\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# Train + Validate + pick best\n",
    "# ----------------------------\n",
    "results = {}\n",
    "best_name = None\n",
    "best_auc = -1.0\n",
    "best_pipe = None\n",
    "\n",
    "for name, clf in candidates.items():\n",
    "    print(f\"\\n=== Training: {name} (with L1 feature selection) ===\")\n",
    "\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"prep\", preprocess),\n",
    "        (\"select\", selector),\n",
    "        (\"model\", clf)\n",
    "    ])\n",
    "\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    val_proba = pipe.predict_proba(X_val)[:, 1]\n",
    "    val_auc = roc_auc_score(y_val, val_proba)\n",
    "    val_ap = average_precision_score(y_val, val_proba)\n",
    "\n",
    "    # How many transformed features were kept?\n",
    "    support = pipe.named_steps[\"select\"].get_support()\n",
    "    kept = int(support.sum())\n",
    "    total = int(support.shape[0])\n",
    "\n",
    "    results[name] = {\n",
    "        \"val_roc_auc\": float(val_auc),\n",
    "        \"val_pr_auc\": float(val_ap),\n",
    "        \"selected_features_kept\": kept,\n",
    "        \"selected_features_total\": total,\n",
    "        \"selected_features_ratio\": float(kept / total) if total else None\n",
    "    }\n",
    "\n",
    "    print(f\"Val ROC-AUC: {val_auc:.6f} | Val PR-AUC: {val_ap:.6f}\")\n",
    "    print(f\"Selected {kept}/{total} transformed features ({kept/total:.1%})\")\n",
    "\n",
    "    if val_auc > best_auc:\n",
    "        best_auc = val_auc\n",
    "        best_name = name\n",
    "        best_pipe = pipe\n",
    "\n",
    "print(f\"\\n✅ Best model: {best_name} | Val ROC-AUC: {best_auc:.6f}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Final test evaluation (best model)\n",
    "# ----------------------------\n",
    "test_proba = best_pipe.predict_proba(X_test)[:, 1]\n",
    "test_auc = roc_auc_score(y_test, test_proba)\n",
    "test_ap = average_precision_score(y_test, test_proba)\n",
    "\n",
    "threshold = 0.5\n",
    "test_pred = (test_proba >= threshold).astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_test, test_pred)\n",
    "report = classification_report(y_test, test_pred, digits=4)\n",
    "\n",
    "print(\"\\n=== Test Metrics (Best Model) ===\")\n",
    "print(f\"Test ROC-AUC: {test_auc:.6f}\")\n",
    "print(f\"Test PR-AUC : {test_ap:.6f}\")\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "\n",
    "results[\"best\"] = {\n",
    "    \"name\": best_name,\n",
    "    \"threshold\": threshold,\n",
    "    \"test_roc_auc\": float(test_auc),\n",
    "    \"test_pr_auc\": float(test_ap),\n",
    "    \"confusion_matrix\": cm.tolist(),\n",
    "    \"classification_report\": report\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# Save artifacts\n",
    "# ----------------------------\n",
    "MODEL_PATH = ART_DIR / \"stage1_pipeline.pkl\"\n",
    "METRICS_PATH = ART_DIR / \"stage1_metrics.json\"\n",
    "META_PATH = ART_DIR / \"metadata.json\"\n",
    "\n",
    "joblib.dump(best_pipe, MODEL_PATH)\n",
    "with open(METRICS_PATH, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\n✅ Saved Stage 1 pipeline to: {MODEL_PATH}\")\n",
    "print(f\"✅ Saved Stage 1 metrics to : {METRICS_PATH}\")\n",
    "\n",
    "# ----------------------------\n",
    "# UI metadata from TRAIN ONLY (no leakage)\n",
    "# numeric: min/max/p1/p99/recommended_min/recommended_max\n",
    "# categorical: top categories\n",
    "# ----------------------------\n",
    "def numeric_meta(series: pd.Series):\n",
    "    s = pd.to_numeric(series, errors=\"coerce\").dropna()\n",
    "    if len(s) == 0:\n",
    "        return None\n",
    "    return {\n",
    "        \"min\": float(s.min()),\n",
    "        \"max\": float(s.max()),\n",
    "        \"p1\": float(np.percentile(s, 1)),\n",
    "        \"p99\": float(np.percentile(s, 99)),\n",
    "        \"recommended_min\": float(np.percentile(s, 10)),\n",
    "        \"recommended_max\": float(np.percentile(s, 90)),\n",
    "    }\n",
    "\n",
    "meta = {\"numeric\": {}, \"categorical\": {}}\n",
    "\n",
    "for c in X_train.columns:\n",
    "    if c in num_cols:\n",
    "        m = numeric_meta(X_train[c])\n",
    "        if m:\n",
    "            meta[\"numeric\"][c] = m\n",
    "    else:\n",
    "        vals = (\n",
    "            X_train[c]\n",
    "            .dropna()\n",
    "            .astype(str)\n",
    "            .value_counts()\n",
    "            .head(30)\n",
    "            .index\n",
    "            .tolist()\n",
    "        )\n",
    "        meta[\"categorical\"][c] = vals\n",
    "\n",
    "with open(META_PATH, \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(f\"✅ Saved UI metadata to     : {META_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
